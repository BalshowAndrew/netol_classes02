{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc05b935",
   "metadata": {},
   "source": [
    "# Класс tf.Module для определения моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacdafc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 21:01:02.654299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754676062.686977   11973 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754676062.694748   11973 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754676062.711603   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754676062.711645   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754676062.711647   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754676062.711649   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0dd80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN(tf.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super().__init__()\n",
    "        self.outputs = outputs\n",
    "        self.fl_init = False\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.fl_init:\n",
    "            self.w = tf.random.truncated_normal((x.shape[-1], self.outputs), stddev=0.1, name=\"w\")\n",
    "            self.b = tf.zeros([self.outputs], dtype=tf.float32, name='b')\n",
    "\n",
    "            self.w = tf.Variable(self.w)\n",
    "            self.b = tf.Variable(self.b)\n",
    "\n",
    "            self.ft_init = True\n",
    "\n",
    "        y = x @ self.w + self.b\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdfe04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNN(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff06cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.175012]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1754676067.376104   11973 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1754676067.377251   11973 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print(model(tf.constant([[1.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4ab9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.random.uniform(minval=0, maxval=10, shape=(100, 2))\n",
    "y_train = [a + b for a, b in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f33330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.9736147 , 5.2648745 ],\n",
       "       [9.438627  , 4.919257  ],\n",
       "       [8.537796  , 9.513378  ],\n",
       "       [9.0712185 , 0.83719015],\n",
       "       [3.5380077 , 5.8911157 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ccdc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=12.238489151000977>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=14.357884407043457>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=18.05117416381836>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.908409118652344>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.429122924804688>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c76f800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda x, y: tf.reduce_mean(tf.square(x - y))\n",
    "opt = tf.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b7a56c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m         f_loss = loss(y, model(x))\n\u001b[32m     10\u001b[39m     grads = tape.gradient(f_loss, model.trainable_variables)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(f_loss.numpy())            \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/netol_classes/.venv/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:463\u001b[39m, in \u001b[36mBaseOptimizer.apply_gradients\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[32m    462\u001b[39m     grads, trainable_variables = \u001b[38;5;28mzip\u001b[39m(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterations\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/netol_classes/.venv/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:504\u001b[39m, in \u001b[36mBaseOptimizer.apply\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    502\u001b[39m             \u001b[38;5;28mself\u001b[39m.build(trainable_variables)\n\u001b[32m    503\u001b[39m         \u001b[38;5;28mself\u001b[39m.built = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_variables_are_known\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m backend.name_scope(\u001b[38;5;28mself\u001b[39m.name, caller=\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    507\u001b[39m     \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[32m    508\u001b[39m     grads, trainable_variables = \u001b[38;5;28mself\u001b[39m._filter_empty_gradients(\n\u001b[32m    509\u001b[39m         grads, trainable_variables\n\u001b[32m    510\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/netol_classes/.venv/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:409\u001b[39m, in \u001b[36mBaseOptimizer._check_variables_are_known\u001b[39m\u001b[34m(self, variables)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainable_variables_indices:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    410\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This optimizer can only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    411\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbe called for the variables it was originally built with. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mWhen working with a new set of variables, you should \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    413\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrecreate a new optimizer instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    414\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Unknown variable: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for n in range(EPOCHS):\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        x = tf.expand_dims(x, axis=0)\n",
    "        y = tf.constant(y, shape=(1, 1))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        f_loss = loss(y, model(x))\n",
    "\n",
    "    grads = tape.gradient(f_loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "print(f_loss.numpy())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c882a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(x_train, y_train):\n",
    "        x = tf.expand_dims(x, axis=0)\n",
    "        y = tf.constant(y, shape=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78d86f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[3.2814097, 3.103248 ]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f84e901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[6.384658]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c10b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-kernel",
   "language": "python",
   "name": "new-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
